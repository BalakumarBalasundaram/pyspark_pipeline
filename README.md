# pyspark_pipeline
### Installation
Clone the repo
   ```sh
   git clone https://github.com/nagarajuerigi/pyspark_pipeline.git
   ```
## Pre Processing steps

1. Upload the metamodel lookup file and data file to data folder
2. Upload notebook process_csv.ipynb to your work space in Databricks Community Edittion to get started
3. It creates the Landing folder, Raw and base locations
4. Copies the file from Data folder to Landing folder

#Start adding more components/functionalities.

# Working on package/library creation in feature branches with our Hackathon team
