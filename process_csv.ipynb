{"cells":[{"cell_type":"code","source":["#define parameters\nINP_SRC_SYS=\"testmax\"\nSRC_STREAM_ID=\"TESTMAX001\"\nPARTITION_DATE=\"FROM_FILE\"\nWRITE_MODE=\"Overwrite\"\n#ENV=\"dev\"\nDBFS_URL=\"dbfs:/\"          #+ENV\nimport datetime\ndate_time=datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\nCREATE_DATE= datetime.datetime.now().strftime(\"%Y-%m-%d\")\nprint(CREATE_DATE)\ncurrent_year=CREATE_DATE.split(\"-\")[0].lstrip('0')\ncurrent_month=CREATE_DATE.split(\"-\")[1].lstrip('0')\ncurrent_day=CREATE_DATE.split(\"-\")[2].lstrip('0')\nBATCH_ID = SRC_STREAM_ID+\"_\"+date_time\nprint(BATCH_ID)\nprint(date_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96b06e66-7778-4a50-953e-9bb19830d5cc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"2022-03-25\nTESTMAX001_20220325081304\n20220325081304\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["2022-03-25\nTESTMAX001_20220325081304\n20220325081304\n"]}}],"execution_count":0},{"cell_type":"code","source":["import os\nimport re\ndef db_list_files(file_path, file_prefix):\n  #file_list = [file.path for file in dbutils.fs.ls(file_path) if os.path.basename(file.path).startswith(file_prefix)]\n  file_list = [file.path for file in dbutils.fs.ls(file_path) if re.search(file_prefix, os.path.basename(file.path))]\n  return file_list\n#db_list_files(DBFS_URL+\"/tmp/data/payment/bgmax/\", \"^bgmax_bettdfi_[0-9]{8}_[0-9]\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb63b03a-566d-40a2-8caa-3be03d092580"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def meta_model(INP_SRC_SYS):\n  try:\n    metamodel_full_df = (spark.read           # The DataFrameReader\n       .option(\"sep\", \"|\")        # Use tab delimiter (default is comma-separator)\n      #.option(\"header\", \"true\")   # Use first line of all files as header\n       .schema(METAMODEL_SCHEMA)   #Apply schema\n       .csv(METAMODEL_LOOKUP)               # Creates a DataFrame from CSV after reading in the file\n    )\n    #metamodel_full_df.show(vertical=True)\n    metamodel_df=metamodel_full_df.filter(metamodel_full_df.SRC_STREAM_ID==SRC_STREAM_ID)\n    #metamodel_df.show(vertical=True)\n    return metamodel_df\n  except:\n    print(\"Metamodel doesnt exists for this source\")\n#metamodel_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7970bfd2-ed96-4bf8-a2a7-760f01218fc5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def file_exists(path):\n  try:\n    dbutils.fs.ls(path)\n    #print(\"The path \",path,\" exists\")\n    pass\n    return True\n  except:\n    print(\"The path \",path,\" does not exist\")\n    return False"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e21a8e3-ae1d-41de-ac60-48b7d6f174bd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def create_partions_from_file(input_dir,part_keys):\n  #part_keys=\"ing_year=YYYY:ing_month=MM:ing_day=DD\"\n  partition_values = {}\n  partitons=part_keys.split(\":\")\n  #len=len(xx)\n  partition_k = input_dir\n  for i in partitons:\n    #print(i)\n    if i == 'ing_year=YYYY' :\n      partition_k = partition_k + '/ing_year='+current_year\n      partition_values['ing_year']=current_year\n      return_value = partition_values\n      #if not file_exists(partition_k):\n       # print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n    elif i == 'ing_month=MM':\n      partition_k = partition_k + '/ing_month='+current_month\n      partition_values['ing_month']=current_month\n      return_value = partition_values\n      #if not file_exists(partition_k):\n       # print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n    elif i == 'ing_day=DD':\n      partition_k = partition_k + '/ing_day='+current_day\n      partition_values['ing_day']=current_day\n      return_value = partition_values\n      #if not file_exists(partition_k):\n       # print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n    elif i == 'create_dir':\n      print(\"Creating input_dir\", partition_k)\n      dbutils.fs.mkdirs(partition_k)\n      return_value = partition_k\n  #print(\"partition_k\",partition_k)\n  #print(\"partition_values\",partition_values)\n  return return_value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20cbcd1c-80dc-44e3-965f-4d7f999607dd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def create_partions_with_current_date(input_dir,part_keys):\n  #part_keys=\"ing_year=YYYY:ing_month=MM:ing_day=DD\"\n  partition_values = {}\n  partitons=part_keys.split(\":\")\n  #len=len(xx)\n  partition_k = input_dir\n  for i in partitons:\n    #print(i)\n    if i == 'ing_year=YYYY' :\n      partition_k = partition_k + '/ing_year='+current_year\n      partition_values['ing_year']=current_year\n      return_value = partition_values\n      #if not file_exists(partition_k):\n       # print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n    elif i == 'ing_month=MM':\n      partition_k = partition_k + '/ing_month='+current_month\n      partition_values['ing_month']=current_month\n      return_value = partition_values\n      #if not file_exists(partition_k):\n       # print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n    elif i == 'ing_day=DD':\n      partition_k = partition_k + '/ing_day='+current_day\n      partition_values['ing_day']=current_day\n      return_value = partition_values\n      #if not file_exists(partition_k):\n       # print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n    elif i == 'create_dir':\n      print(\"Creating input_dir\", partition_k)\n      dbutils.fs.mkdirs(partition_k)\n      return_value = partition_k\n  #print(\"partition_k\",partition_k)\n  #print(\"partition_values\",partition_values)\n  return return_value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7521ac27-2004-41ef-9a96-ca3f73c1147e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_metamodel_schema():\n  METAMODEL_SCHEMA = StructType([ \\\n    StructField(\"PROCESS_ID\",StringType(),True), \\\n    StructField(\"PROCESS_TYPE\",StringType(),True), \\\n    StructField(\"SRC_ID\",StringType(),True), \\\n    StructField(\"SRC_SYS_NM\",StringType(),True), \\\n    StructField(\"STREAM_NM\",StringType(),True), \\\n    StructField(\"SRC_STREAM_ID\",StringType(),True), \\\n    StructField(\"SRC_SYS_TYPE\",StringType(),True), \\\n    StructField(\"SRC_SYS_DESC\",StringType(),True), \\\n    StructField(\"FIELD_SEP\",StringType(),True), \\\n    StructField(\"LINE_SEP\",StringType(),True), \\\n    StructField(\"HEADER_ROW\",StringType(),True), \\\n    StructField(\"FOOTER_ROW\",StringType(),True), \\\n    StructField(\"LOAD_TYPE\",StringType(),True), \\\n    StructField(\"WHERE_CLUASE\",StringType(),True), \\\n    StructField(\"PARTITION_KEY\",StringType(),True), \\\n    StructField(\"CUST_DATA_FLG\",StringType(),True), \\\n    StructField(\"CUST_DATA_DESC\",StringType(),True), \\\n    StructField(\"LEGAL_GRAND\",StringType(),True), \\\n    StructField(\"LOAD_FREQ\",StringType(),True), \\\n    StructField(\"LANDING_DIR\",StringType(),True), \\\n    StructField(\"RAW_ODL_LOC\",StringType(),True), \\\n    StructField(\"BASE_TBL_NAME\",StringType(),True), \\\n    StructField(\"BASE_ODL_LOC\",StringType(),True), \\\n    StructField(\"SCHEMA_DIR\",StringType(),True), \\\n    StructField(\"C1_TBL_NAME\",StringType(),True), \\\n    StructField(\"C1_ODL_LOC\",StringType(),True), \\\n    StructField(\"DELIVERY_FORMAT\",StringType(),True), \\\n    StructField(\"SRV_NAME\",StringType(),True), \\\n    StructField(\"SRV_SHRT_NAME\",StringType(),True), \\\n    StructField(\"SRC_TIME_ZONE\",StringType(),True) \\\n  ])\n  return METAMODEL_SCHEMA"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed6acb6d-f146-4f3d-b098-483edf5e514c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#define Dropzone to Raw pipeline\ndef dropzone_to_raw(INP_DF):\n  print(\"###########################DropZone to RAW Pipeline ##################################\")\n  #INP_DF.show(vertical=True)\n  #input_file=INP_DF.select(\"STREAM_NM\").rdd.flatMap(list).collect()\n  input_file=INP_DF.select(\"STREAM_NM\").rdd.flatMap(list).collect()[0]\n  LANDING_DIR=INP_DF.select(\"LANDING_DIR\").rdd.flatMap(list).collect()[0]\n  #dbfs_land_dir=LANDING_DIR.replace(\"/mnt/shareddisk\",DBFS_URL+\"/tmp\")\n  dbfs_land_dir=LANDING_DIR\n  print(\"dbfs_land_dir:\",dbfs_land_dir)\n  RAW_ODL_LOC=INP_DF.select(\"RAW_ODL_LOC\").rdd.flatMap(list).collect()[0]\n  print(\"RAW_ODL_LOC:\",RAW_ODL_LOC)\n  dbfs_raw_file=DBFS_URL+\"odl1/raw\" + RAW_ODL_LOC + CREATE_DATE.replace(\"-\",'')\n  \n  print(\"input file name is :\",input_file)\n  print(\"Landing directory is :\",LANDING_DIR)\n  print(\"Raw directory is :\",RAW_ODL_LOC)\n  print(\"Databricks Raw Loc is :\",dbfs_raw_file)\n  #File listing\n  #print(\"File listing\",dbutils.fs.ls(dbfs_land_dir))\n  landing_file = db_list_files(dbfs_land_dir, input_file)[0]\n  raw_file_name = landing_file.split(os.sep)[-1]\n  #check if directory exists, else create it\n  if not file_exists(dbfs_raw_file):\n    print(\"Creating directory \",dbfs_raw_file,\"now\")\n    dbutils.fs.mkdirs(dbfs_raw_file)\n  dbfs_raw_file = dbfs_raw_file +\"/\"+ raw_file_name\n  print(\"RAW FILE:\",dbfs_raw_file)\n  if not file_exists(dbfs_raw_file):\n  # Copy the file from Landing directory to Raw folder  \n    print(\"Copying the file \",landing_file,\"to Raw ADLS \",dbfs_raw_file)\n    dbutils.fs.cp(landing_file,dbfs_raw_file)\n  print(\"###########################DropZone to RAW Pipeline Completed Successfully##################################\")\n    \n  return dbfs_raw_file"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c91c9065-a401-4c2e-bee1-77cb2ff2357b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#define Raw to Base pipeline\nfrom pyspark.sql.functions import lit\ndef raw_to_base_csv(INP_DF,INP_FILE):\n  print(\"###########################RAW to BASE Pipeline ##################################\")\n  #INP_DF.show(vertical=True)\n  #input_file=INP_DF.select(\"STREAM_NM\").rdd.flatMap(list).collect()\n  BASE_ODL_LOC=INP_DF.select(\"BASE_ODL_LOC\").rdd.flatMap(list).collect()[0]\n  BASE_TBL_NAME = INP_DF.select(\"BASE_TBL_NAME\").rdd.flatMap(list).collect()[0]\n  PARTITION_KEY = INP_DF.select(\"PARTITION_KEY\").rdd.flatMap(list).collect()[0]\n  FIELD_SEP = INP_DF.select(\"FIELD_SEP\").rdd.flatMap(list).collect()[0]\n  if FIELD_SEP == 'PIPE':    FIELD_SEP = '|'\n  else: FIELD_SEP\n  dbfs_base_dir=DBFS_URL+\"odl1/base\" + BASE_ODL_LOC + BASE_TBL_NAME.split(\"_\")[1]\n  \n  #Create partition directories\n  if PARTITION_DATE=='FROM_FILE':\n    base_dir=create_partions_from_file(dbfs_base_dir,\"create_dir\")\n    #get partition values\n    partition_values=create_partions_from_file(dbfs_base_dir,PARTITION_KEY)\n  else:\n    base_dir=create_partions_with_current_date(dbfs_base_dir,\"create_dir\")\n    #get partition values\n    partition_values=create_partions_from_file(dbfs_base_dir,PARTITION_KEY)\n\n  \n  print(\"BASE_TBL_NAME  is :\",BASE_TBL_NAME)\n  print(\"BASE_ODL_LOC  is :\",dbfs_base_dir)\n  print(\"PARTITION_KEY  is :\",PARTITION_KEY)\n  print(\"Base Dir is :\",base_dir)\n  print(\"INP_FILE is :\",INP_FILE)\n  print(\"FIELD_SEP is :\",FIELD_SEP)\n    \n  \n  #Read Input file\n  raw_df = (spark.read\n            .option(\"sep\",FIELD_SEP)\n            .option(\"header\",True)\n            .csv(INP_FILE)\n            )\n  #raw_df.show()\n  repartitionedDF = raw_df.repartition(4)\n  enrich_df=repartitionedDF.select(lit(SRC_STREAM_ID).alias(\"src_stream_id\"),\n                          lit(BATCH_ID).alias(\"batch_id\"),\n                          lit(CREATE_DATE).alias(\"create_date\"),\n                          lit(\"UTC\").alias(\"time_zone\"),\n                          lit(CREATE_DATE).alias(\"src_business_date\"),\n                          \"*\")\n  #enrich_df.show()\n  print(\"partition_values:\",partition_values)\n  parts=\"\"\n  for k in partition_values:\n  #  print(k,d[k])\n  #  print(k)\n    parts=parts+\"/\"+k+\"=\"+partition_values[k]\n    enrich_df=enrich_df.withColumn(k,lit(partition_values[k]))\n  output_schema=enrich_df.schema\n  output_file=base_dir+parts\n  print(\"Writing file \", output_file, \"with \",enrich_df.rdd.getNumPartitions(),\" partitions\")\n  enrich_df.write.format(\"avro\").partitionBy(list(partition_values.keys())).mode(WRITE_MODE).save(base_dir)\n  print(\"###########################RAW to BASE Pipeline Completed Succefully ##################################\")\n \n  return output_file\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9032ce5-690c-4d9f-9a9b-d2c0d0e481e2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Metamodel lookup reading\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\n#import sys\n#defining metamodel Schema\nMETAMODEL_SCHEMA=get_metamodel_schema()\ntry:\n  METAMODEL_LOOKUP = DBFS_URL+\"mnt/shareddisk/data/lookup/\"+INP_SRC_SYS+\"_meta_config.lkp\"\n  #Read lookup files\n  #Call Metamodel to fetch data from metamodel lookup\n  metamodel_df=meta_model(INP_SRC_SYS)\n  #metamodel_df.show(vertical=True)\n  metamodel_src2dropzone_df=metamodel_df.filter(metamodel_df.PROCESS_TYPE==\"SOURCE_TO_DROPZONE\")\n  metamodel_dropzone2raw_df=metamodel_df.filter(metamodel_df.PROCESS_TYPE==\"DROPZONE_TO_RAW\")\n  metamodel_raw2base_df=metamodel_df.filter(metamodel_df.PROCESS_TYPE==\"RAW_TO_BASE\")\n  #Dropzone to Raw\n  dbfs_raw_file = dropzone_to_raw(metamodel_dropzone2raw_df)\n  #Raw to Base\n  BASE_ODL_LOC=metamodel_raw2base_df.select(\"BASE_ODL_LOC\").rdd.flatMap(list).collect()[0]\n  BASE_TBL_NAME = metamodel_raw2base_df.select(\"BASE_TBL_NAME\").rdd.flatMap(list).collect()[0]\n  dbfs_base_dir=DBFS_URL+\"odl1/base\" + BASE_ODL_LOC + BASE_TBL_NAME.split(\"_\")[1]\n  output_file=raw_to_base_csv(metamodel_raw2base_df,dbfs_raw_file)\n  #Move raw file to Processed Folder\n  \n  #metamodel_raw2base_df.show(vertical=True)\n\nexcept:\n  import sys\n  print(\"Metamodel lookup file\",METAMODEL_LOOKUP,\" doesnt exists for this source\",sys.exc_info()[0], \"occurred.\")\n  #print(\"Metamodel lookup file\",METAMODEL_LOOKUP,\" doesnt exists for this source\")\n        #ys.exc_info()[0], \"occurred.\")\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"461585a0-6646-45b6-9661-e557a9217f79"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"###########################DropZone to RAW Pipeline ##################################\ndbfs_land_dir: /mnt/shareddisk/data/test/testmax/\nRAW_ODL_LOC: /data/test/testmax/tstdfi/\ninput file name is : ^testmax_tstdfi_[0-9]{8}_[0-9]\nLanding directory is : /mnt/shareddisk/data/test/testmax/\nRaw directory is : /data/test/testmax/tstdfi/\nDatabricks Raw Loc is : dbfs:/odl1/raw/data/test/testmax/tstdfi/20220325\nRAW FILE: dbfs:/odl1/raw/data/test/testmax/tstdfi/20220325/testmax_tstdfi_20220301_20220301154848\n###########################DropZone to RAW Pipeline Completed Successfully##################################\n###########################RAW to BASE Pipeline ##################################\nCreating input_dir dbfs:/odl1/base/data/test/testmax/tstdfi\nBASE_TBL_NAME  is : testmax_tstdfi\nBASE_ODL_LOC  is : dbfs:/odl1/base/data/test/testmax/tstdfi\nPARTITION_KEY  is : ing_year=YYYY:ing_month=MM:ing_day=DD\nBase Dir is : dbfs:/odl1/base/data/test/testmax/tstdfi\nINP_FILE is : dbfs:/odl1/raw/data/test/testmax/tstdfi/20220325/testmax_tstdfi_20220301_20220301154848\nFIELD_SEP is : |\npartition_values: {'ing_year': '2022', 'ing_month': '3', 'ing_day': '25'}\nWriting file  dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25 with  4  partitions\n###########################RAW to BASE Pipeline Completed Succefully ##################################\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["###########################DropZone to RAW Pipeline ##################################\ndbfs_land_dir: /mnt/shareddisk/data/test/testmax/\nRAW_ODL_LOC: /data/test/testmax/tstdfi/\ninput file name is : ^testmax_tstdfi_[0-9]{8}_[0-9]\nLanding directory is : /mnt/shareddisk/data/test/testmax/\nRaw directory is : /data/test/testmax/tstdfi/\nDatabricks Raw Loc is : dbfs:/odl1/raw/data/test/testmax/tstdfi/20220325\nRAW FILE: dbfs:/odl1/raw/data/test/testmax/tstdfi/20220325/testmax_tstdfi_20220301_20220301154848\n###########################DropZone to RAW Pipeline Completed Successfully##################################\n###########################RAW to BASE Pipeline ##################################\nCreating input_dir dbfs:/odl1/base/data/test/testmax/tstdfi\nBASE_TBL_NAME  is : testmax_tstdfi\nBASE_ODL_LOC  is : dbfs:/odl1/base/data/test/testmax/tstdfi\nPARTITION_KEY  is : ing_year=YYYY:ing_month=MM:ing_day=DD\nBase Dir is : dbfs:/odl1/base/data/test/testmax/tstdfi\nINP_FILE is : dbfs:/odl1/raw/data/test/testmax/tstdfi/20220325/testmax_tstdfi_20220301_20220301154848\nFIELD_SEP is : |\npartition_values: {'ing_year': '2022', 'ing_month': '3', 'ing_day': '25'}\nWriting file  dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25 with  4  partitions\n###########################RAW to BASE Pipeline Completed Succefully ##################################\n"]}}],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.ls(output_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"994c2181-8a1e-42e9-b116-1e53ecb5253c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[11]: [FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1648196013000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/_committed_2675974476602157253', name='_committed_2675974476602157253', size=380, modificationTime=1648196012000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/_started_2675974476602157253', name='_started_2675974476602157253', size=0, modificationTime=1648196010000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/part-00000-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-12-1.c000.avro', name='part-00000-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-12-1.c000.avro', size=997, modificationTime=1648196011000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/part-00001-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-13-1.c000.avro', name='part-00001-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-13-1.c000.avro', size=976, modificationTime=1648196011000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/part-00002-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-14-1.c000.avro', name='part-00002-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-14-1.c000.avro', size=978, modificationTime=1648196011000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/part-00003-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-15-1.c000.avro', name='part-00003-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-15-1.c000.avro', size=1013, modificationTime=1648196011000)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: [FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1648196013000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/_committed_2675974476602157253', name='_committed_2675974476602157253', size=380, modificationTime=1648196012000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/_started_2675974476602157253', name='_started_2675974476602157253', size=0, modificationTime=1648196010000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/part-00000-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-12-1.c000.avro', name='part-00000-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-12-1.c000.avro', size=997, modificationTime=1648196011000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/part-00001-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-13-1.c000.avro', name='part-00001-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-13-1.c000.avro', size=976, modificationTime=1648196011000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/part-00002-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-14-1.c000.avro', name='part-00002-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-14-1.c000.avro', size=978, modificationTime=1648196011000),\n FileInfo(path='dbfs:/odl1/base/data/test/testmax/tstdfi/ing_year=2022/ing_month=3/ing_day=25/part-00003-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-15-1.c000.avro', name='part-00003-tid-2675974476602157253-b928a120-b8e3-4c8f-8396-e084ff02a712-15-1.c000.avro', size=1013, modificationTime=1648196011000)]"]}}],"execution_count":0},{"cell_type":"code","source":["#Verify output file\nprint(dbfs_base_dir)\nbase_df = spark.read.format(\"avro\").load(dbfs_base_dir)\nbase_df.show()          \n             "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fcdfdb6-0bf1-45f3-8d50-a8bdfa62dec3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"dbfs:/odl1/base/data/test/testmax/tstdfi\n+-------------+--------------------+-----------+---------+-----------------+-------+--------------------+------------+--------------------+------------+----------------------+---------------------+----------------------+--------+---------+-------+\n|src_stream_id|            batch_id|create_date|time_zone|src_business_date|FILE_ID|         LAYOUT_NAME|FILE_VERSION|           TIMESTAMP|CREDIT_ROUND|NUM_OF_PAYMENT_RECORDS|NUM_OF_DEDUCT_RECORDS|NUM_OF_DEPOSIT_RECORDS|ing_year|ing_month|ing_day|\n+-------------+--------------------+-----------+---------+-----------------+-------+--------------------+------------+--------------------+------------+----------------------+---------------------+----------------------+--------+---------+-------+\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|     61|TESTNSUPP FIL    ...|          01|2016-04-28-08.45....|       TST56|                     1|                    1|                     1|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    112|TESTNSUPP FIL    ...|          01|2016-05-17-07.49....|       TST37|                     8|                    2|                     4|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    148|TESTNSUPP FIL    ...|          01|2016-05-18-07.49....|       TST37|                     8|                    2|                     4|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    122|TESTNSUPP FIL    ...|          01|2016-05-02-08.48....|       TST37|                     1|                    1|                     1|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|     41|TESTNSUPP FIL    ...|          01|2016-04-25-08.45....|       TST56|                     1|                    0|                     1|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    144|TESTNSUPP FIL    ...|          01|2016-05-12-07.48....|       TST37|                     8|                    1|                     3|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    109|TESTNSUPP FIL    ...|          01|2016-05-11-07.48....|       TST37|                     8|                    1|                     3|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    107|TESTNSUPP FIL    ...|          01|2016-05-03-09.54....|       TST37|                     1|                    1|                     1|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    128|TESTNSUPP FIL    ...|          01|2016-05-06-09.54....|       TST37|                     1|                    1|                     1|    2022|        3|     25|\n+-------------+--------------------+-----------+---------+-----------------+-------+--------------------+------------+--------------------+------------+----------------------+---------------------+----------------------+--------+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["dbfs:/odl1/base/data/test/testmax/tstdfi\n+-------------+--------------------+-----------+---------+-----------------+-------+--------------------+------------+--------------------+------------+----------------------+---------------------+----------------------+--------+---------+-------+\n|src_stream_id|            batch_id|create_date|time_zone|src_business_date|FILE_ID|         LAYOUT_NAME|FILE_VERSION|           TIMESTAMP|CREDIT_ROUND|NUM_OF_PAYMENT_RECORDS|NUM_OF_DEDUCT_RECORDS|NUM_OF_DEPOSIT_RECORDS|ing_year|ing_month|ing_day|\n+-------------+--------------------+-----------+---------+-----------------+-------+--------------------+------------+--------------------+------------+----------------------+---------------------+----------------------+--------+---------+-------+\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|     61|TESTNSUPP FIL    ...|          01|2016-04-28-08.45....|       TST56|                     1|                    1|                     1|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    112|TESTNSUPP FIL    ...|          01|2016-05-17-07.49....|       TST37|                     8|                    2|                     4|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    148|TESTNSUPP FIL    ...|          01|2016-05-18-07.49....|       TST37|                     8|                    2|                     4|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    122|TESTNSUPP FIL    ...|          01|2016-05-02-08.48....|       TST37|                     1|                    1|                     1|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|     41|TESTNSUPP FIL    ...|          01|2016-04-25-08.45....|       TST56|                     1|                    0|                     1|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    144|TESTNSUPP FIL    ...|          01|2016-05-12-07.48....|       TST37|                     8|                    1|                     3|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    109|TESTNSUPP FIL    ...|          01|2016-05-11-07.48....|       TST37|                     8|                    1|                     3|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    107|TESTNSUPP FIL    ...|          01|2016-05-03-09.54....|       TST37|                     1|                    1|                     1|    2022|        3|     25|\n|   TESTMAX001|TESTMAX001_202203...| 2022-03-25|      UTC|       2022-03-25|    128|TESTNSUPP FIL    ...|          01|2016-05-06-09.54....|       TST37|                     1|                    1|                     1|    2022|        3|     25|\n+-------------+--------------------+-----------+---------+-----------------+-------+--------------------+------------+--------------------+------------+----------------------+---------------------+----------------------+--------+---------+-------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"process_csv","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4017665550194019}},"nbformat":4,"nbformat_minor":0}
