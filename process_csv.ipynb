{"cells":[{"cell_type":"code","source":["#define parameters\nINP_SRC_SYS=\"bgmax\"\nSRC_STREAM_ID=\"BGMAX001\"\nPARTITION_DATE=\"FROM_FILE\"\nWRITE_MODE=\"Overwrite\"\nENV=\"dev\"\nAZURE_URL=\"dbfs://\"+ENV\nimport datetime\ndate_time=datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\nCREATE_DATE= datetime.datetime.now().strftime(\"%Y-%m-%d\")\nprint(CREATE_DATE)\ncurrent_year=CREATE_DATE.split(\"-\")[0].lstrip('0')\ncurrent_month=CREATE_DATE.split(\"-\")[1].lstrip('0')\ncurrent_day=CREATE_DATE.split(\"-\")[2].lstrip('0')\nBATCH_ID = SRC_STREAM_ID+\"_\"+date_time\nprint(BATCH_ID)\nprint(date_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96b06e66-7778-4a50-953e-9bb19830d5cc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import os\nimport re\ndef db_list_files(file_path, file_prefix):\n  #file_list = [file.path for file in dbutils.fs.ls(file_path) if os.path.basename(file.path).startswith(file_prefix)]\n  file_list = [file.path for file in dbutils.fs.ls(file_path) if re.search(file_prefix, os.path.basename(file.path))]\n  return file_list\n#db_list_files(AZURE_URL+\"/tmp/data/payment/bgmax/\", \"^bgmax_bettdfi_[0-9]{8}_[0-9]\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb63b03a-566d-40a2-8caa-3be03d092580"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def meta_model(INP_SRC_SYS):\n  try:\n    metamodel_full_df = (spark.read           # The DataFrameReader\n       .option(\"sep\", \"|\")        # Use tab delimiter (default is comma-separator)\n      #.option(\"header\", \"true\")   # Use first line of all files as header\n       .schema(METAMODEL_SCHEMA)   #Apply schema\n       .csv(METAMODEL_LOOKUP)               # Creates a DataFrame from CSV after reading in the file\n    )\n    #metamodel_full_df.show(vertical=True)\n    metamodel_df=metamodel_full_df.filter(metamodel_full_df.SRC_STREAM_ID==SRC_STREAM_ID)\n    metamodel_df.show(vertical=True)\n    return metamodel_df\n  except:\n    print(\"Metamodel doesnt exists for this source\")\n#metamodel_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7970bfd2-ed96-4bf8-a2a7-760f01218fc5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def file_exists(path):\n  try:\n    dbutils.fs.ls(path)\n    print(\"The path \",path,\" exists\")\n    pass\n    return True\n  except:\n    print(\"The path \",path,\" does not exist\")\n    return False"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e21a8e3-ae1d-41de-ac60-48b7d6f174bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#define Dropzone to Raw pipeline\ndef dropzone_to_raw(INP_DF):\n  print(\"###########################DropZone to RAW Pipeline ##################################\")\n  #INP_DF.show(vertical=True)\n  #input_file=INP_DF.select(\"STREAM_NM\").rdd.flatMap(list).collect()\n  input_file=INP_DF.select(\"STREAM_NM\").rdd.flatMap(list).collect()[0]\n  LANDING_DIR=INP_DF.select(\"LANDING_DIR\").rdd.flatMap(list).collect()[0]\n  Azure_land_dir=LANDING_DIR.replace(\"/mnt/shareddisk\",AZURE_URL+\"/tmp\")\n  RAW_ODL_LOC=INP_DF.select(\"RAW_ODL_LOC\").rdd.flatMap(list).collect()[0]\n  Azure_raw_dir=AZURE_URL+\"/raw\" + RAW_ODL_LOC + CREATE_DATE.replace(\"-\",'')\n  \n  print(\"input file name is :\",input_file)\n  print(\"Landing directory is :\",LANDING_DIR)\n  print(\"Raw directory is :\",RAW_ODL_LOC)\n  print(\"Azure Raw Loc is :\",Azure_raw_dir)\n  #File listing\n#  print(dbutils.fs.ls(Azure_land_dir))\n  landing_file = db_list_files(Azure_land_dir, input_file)[0]\n  raw_file_name = landing_file.split(os.sep)[-1]\n  #check if directory exists, else create it\n  if not file_exists(Azure_raw_dir):\n    print(\"Creating directory \",Azure_raw_dir,\"now\")\n    dbutils.fs.mkdirs(Azure_raw_dir)\n  Azure_raw_file = Azure_raw_dir +\"/\"+ raw_file_name\n  print(\"RAW FILE:\",Azure_raw_file)\n  if not file_exists(Azure_raw_file):\n  # Copy the file from Landing directory to Raw folder  \n    print(\"Copying the file \",landing_file,\"to Raw ADLS \",Azure_raw_dir)\n    dbutils.fs.cp(landing_file,Azure_raw_dir)\n    \n  return Azure_raw_file"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c91c9065-a401-4c2e-bee1-77cb2ff2357b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def create_partions_with_current_date(base_dir,part_keys):\n  #part_keys=\"ing_year=YYYY:ing_month=MM:ing_day=DD\"\n  partition_values = {}\n  partitons=part_keys.split(\":\")\n  #len=len(xx)\n  partition_k = base_dir\n  for i in partitons:\n    #print(i)\n    if i == 'ing_year=YYYY' :\n      partition_k = partition_k + '/ing_year='+current_year\n      partition_values['ing_year']=current_year\n      if not file_exists(partition_k):\n        print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n    elif i == 'ing_month=MM':\n      partition_k = partition_k + '/ing_month='+current_month\n      partition_values['ing_month']=current_month\n      if not file_exists(partition_k):\n        print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n    elif i == 'ing_day=DD':\n      partition_k = partition_k + '/ing_day='+current_day\n      partition_values['ing_day']=current_day\n      if not file_exists(partition_k):\n        print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n  print(partition_k)\n  print(partition_values)\n  return partition_k"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7521ac27-2004-41ef-9a96-ca3f73c1147e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def create_partions_from_file(base_dir,part_keys):\n  #part_keys=\"ing_year=YYYY:ing_month=MM:ing_day=DD\"\n  partition_values = {}\n  partitons=part_keys.split(\":\")\n  #len=len(xx)\n  partition_k = base_dir\n  for i in partitons:\n    #print(i)\n    if i == 'ing_year=YYYY' :\n      partition_k = partition_k + '/ing_year='+current_year\n      partition_values['ing_year']=current_year\n      if not file_exists(partition_k):\n        print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n    elif i == 'ing_month=MM':\n      partition_k = partition_k + '/ing_month='+current_month\n      partition_values['ing_month']=current_month\n      if not file_exists(partition_k):\n        print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n    elif i == 'ing_day=DD':\n      partition_k = partition_k + '/ing_day='+current_day\n      partition_values['ing_day']=current_day\n      if not file_exists(partition_k):\n        print(\"Creating directory \",partition_k,\"now\")\n        #dbutils.fs.mkdirs(partition_k)\n  print(partition_k)\n  print(partition_values)\n  return partition_k"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20cbcd1c-80dc-44e3-965f-4d7f999607dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#define Raw to Base pipeline\nfrom pyspark.sql.functions import lit\ndef raw_to_base(INP_DF,INP_FILE):\n  print(\"###########################RAW to BASE Pipeline ##################################\")\n  #INP_DF.show(vertical=True)\n  #input_file=INP_DF.select(\"STREAM_NM\").rdd.flatMap(list).collect()\n  BASE_ODL_LOC=INP_DF.select(\"BASE_ODL_LOC\").rdd.flatMap(list).collect()[0]\n  BASE_TBL_NAME = INP_DF.select(\"BASE_TBL_NAME\").rdd.flatMap(list).collect()[0]\n  PARTITION_KEY = INP_DF.select(\"PARTITION_KEY\").rdd.flatMap(list).collect()[0]\n  FIELD_SEP = INP_DF.select(\"FIELD_SEP\").rdd.flatMap(list).collect()[0]\n  Azure_base_dir=AZURE_URL+\"/base\" + BASE_ODL_LOC + BASE_TBL_NAME.split(\"_\")[1]\n  \n  #Create partition directories\n  if PARTITION_DATE=='FROM_FILE':\n    base_dir=create_partions_from_file(Azure_base_dir,PARTITION_KEY)\n  else:\n    base_dir=create_partions_with_current_date(Azure_base_dir,PARTITION_KEY)\n    \n  print(\"BASE_TBL_NAME  is :\",BASE_TBL_NAME)\n  print(\"BASE_ODL_LOC  is :\",Azure_base_dir)\n  print(\"PARTITION_KEY  is :\",PARTITION_KEY)\n  print(\"Base Dir is :\",base_dir)\n  print(\"INP_FILE is :\",INP_FILE)\n  \n  #Read Input file\n  raw_df = (spark.read\n            .option(\"sep\",FIELD_SEP)\n            .option(\"header\",True)\n            .csv(INP_FILE)\n            )\n  #raw_df.show()\n  repartitionedDF = raw_df.repartition(4)\n  enrich_df=repartitionedDF.select(lit(SRC_STREAM_ID).alias(\"src_stream_id\"),\n                          lit(BATCH_ID).alias(\"batch_id\"),\n                          lit(CREATE_DATE).alias(\"create_date\"),\n                          lit(\"UTC\").alias(\"time_zone\"),\n                          lit(CREATE_DATE).alias(\"src_business_date\"),\n                          \"*\")\n  #enrich_df.show()\n  print(\"Writing file \", base_dir, \"with \",enrich_df.rdd.getNumPartitions(),\" partitions\")\n  for k in partition_values:\n    #print(k,d[k])\n    #print(k)\n      enrich_df=enrich_df.withColumn(k,lit(partition_values[k]))\n#enrich_df.printSchema()\n  enrich_df.write.format(\"avro\").partitionBy(list(partition_values.keys())).mode(WRITE_MODE).save(base_dir)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9032ce5-690c-4d9f-9a9b-d2c0d0e481e2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Metamodel lookup reading\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\n#import sys\n#defining metamodel Schema\nMETAMODEL_SCHEMA=StructType([ \\\n    StructField(\"PROCESS_ID\",StringType(),True), \\\n    StructField(\"PROCESS_TYPE\",StringType(),True), \\\n    StructField(\"SRC_ID\",StringType(),True), \\\n    StructField(\"SRC_SYS_NM\",StringType(),True), \\\n    StructField(\"STREAM_NM\",StringType(),True), \\\n    StructField(\"SRC_STREAM_ID\",StringType(),True), \\\n    StructField(\"SRC_SYS_TYPE\",StringType(),True), \\\n    StructField(\"SRC_SYS_DESC\",StringType(),True), \\\n    StructField(\"FIELD_SEP\",StringType(),True), \\\n    StructField(\"LINE_SEP\",StringType(),True), \\\n    StructField(\"HEADER_ROW\",StringType(),True), \\\n    StructField(\"FOOTER_ROW\",StringType(),True), \\\n    StructField(\"LOAD_TYPE\",StringType(),True), \\\n    StructField(\"WHERE_CLUASE\",StringType(),True), \\\n    StructField(\"PARTITION_KEY\",StringType(),True), \\\n    StructField(\"CUST_DATA_FLG\",StringType(),True), \\\n    StructField(\"CUST_DATA_DESC\",StringType(),True), \\\n    StructField(\"LEGAL_GRAND\",StringType(),True), \\\n    StructField(\"LOAD_FREQ\",StringType(),True), \\\n    StructField(\"LANDING_DIR\",StringType(),True), \\\n    StructField(\"RAW_ODL_LOC\",StringType(),True), \\\n    StructField(\"BASE_TBL_NAME\",StringType(),True), \\\n    StructField(\"BASE_ODL_LOC\",StringType(),True), \\\n    StructField(\"SCHEMA_DIR\",StringType(),True), \\\n    StructField(\"C1_TBL_NAME\",StringType(),True), \\\n    StructField(\"C1_ODL_LOC\",StringType(),True), \\\n    StructField(\"DELIVERY_FORMAT\",StringType(),True), \\\n    StructField(\"SRV_NAME\",StringType(),True), \\\n    StructField(\"SRV_SHRT_NAME\",StringType(),True), \\\n    StructField(\"SRC_TIME_ZONE\",StringType(),True) \\\n  ])\n\ntry:\n  METAMODEL_LOOKUP = AZURE_URL+\"/lookup/\"+INP_SRC_SYS+\"_meta_config.lkp\"\n  #Read lookup files\n  #print(dbutils.fs.ls(METAMODEL_LOOKUP))\n  #Call Metamodel to fetch data from metamodel lookup\n  metamodel_df=meta_model(INP_SRC_SYS)\n  #metamodel_df.show(vertical=True)\n  metamodel_src2dropzone_df=metamodel_df.filter(metamodel_df.PROCESS_TYPE==\"SOURCE_TO_DROPZONE\")\n  metamodel_dropzone2raw_df=metamodel_df.filter(metamodel_df.PROCESS_TYPE==\"DROPZONE_TO_RAW\")\n  metamodel_raw2base_df=metamodel_df.filter(metamodel_df.PROCESS_TYPE==\"RAW_TO_BASE\")\n  #Dropzone to Raw\n  Azure_raw_file = dropzone_to_raw(metamodel_dropzone2raw_df)\n  #Raw to Base\n  raw_to_base(metamodel_raw2base_df,Azure_raw_file)\n  #metamodel_raw2base_df.show(vertical=True)\n\nexcept:\n  print(\"Metamodel lookup file\",METAMODEL_LOOKUP,\" doesnt exists for this source\")\n        #ys.exc_info()[0], \"occurred.\")\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"461585a0-6646-45b6-9661-e557a9217f79"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"process_csv","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4017665550194019}},"nbformat":4,"nbformat_minor":0}
